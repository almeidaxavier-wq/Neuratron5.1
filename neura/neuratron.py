import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import random
import typing
import os
import networkx as nx


class Neuratron(nn.Module):
    def __init__(self, total_size: typing.Tuple[int, int]):
        super(Neuratron, self).__init__()
        
        # Inicializa o grande tensor de parÃ¢metros
        self.total_weight = nn.Parameter(torch.Tensor(total_size[1], total_size[0]))
        self.total_bias = nn.Parameter(torch.Tensor(total_size[1]))
        
        # Lista de blocos livres: cada bloco Ã© ((in_start, in_end), (out_start, out_end))
        self.free_blocks = [((0, total_size[0]), (0, total_size[1]))]
        
        # DicionÃ¡rio de alocaÃ§Ãµes
        self.allocations = {}
        
        # InicializaÃ§Ã£o dos pesos
        self._init_parameters()

    def _init_parameters(self) -> None:
        """Inicializa os parÃ¢metros usando Kaiming initialization"""
        # InicializaÃ§Ã£o do peso
        nn.init.kaiming_uniform_(self.total_weight, a=math.sqrt(5))
        
        # InicializaÃ§Ã£o do bias
        if self.total_bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.total_weight)
            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
            nn.init.uniform_(self.total_bias, -bound, bound)

    def allocate(self, name: str, input_shape: int, output_shape: int) -> bool:
        """Aloca um bloco de parÃ¢metros para uma nova camada"""
        
        # Encontrar o melhor bloco livre que caiba a alocaÃ§Ã£o
        best_block_idx = -1
        best_fit_score = float('inf')
        
        for i, ((in_start, in_end), (out_start, out_end)) in enumerate(self.free_blocks):
            in_available = in_end - in_start
            out_available = out_end - out_start
            
            if in_available >= input_shape and out_available >= output_shape:
                # Calcula o "desperdÃ­cio" (quanto sobra)
                waste = (in_available - input_shape) + (out_available - output_shape)
                if waste < best_fit_score:
                    best_fit_score = waste
                    best_block_idx = i
        
        if best_block_idx == -1:
            raise RuntimeError("NÃ£o hÃ¡ espaÃ§o suficiente para alocar")
        
        # Recupera o bloco escolhido
        (in_start, in_end), (out_start, out_end) = self.free_blocks.pop(best_block_idx)
        
        # Define os limites da alocaÃ§Ã£o
        alloc_in_end = in_start + input_shape
        alloc_out_end = out_start + output_shape
        
        # Registra a alocaÃ§Ã£o
        self.allocations[name] = (
            (in_start, alloc_in_end), 
            (out_start, alloc_out_end)
        )
        
        # ğŸ”¥ DIVISÃƒO DO ESPAÃ‡O LIVRE ğŸ”¥
        # Criamos os 3 novos blocos livres resultantes da alocaÃ§Ã£o
        
        # Bloco 1: Direita do alocado (mesma linha, coluna Ã  direita)
        if alloc_in_end < in_end:
            free1 = ((alloc_in_end, in_end), (out_start, out_end))
            self.free_blocks.append(free1)
        
        # Bloco 2: Abaixo do alocado (mesma coluna, linha abaixo)  
        if alloc_out_end < out_end:
            free2 = ((in_start, in_end), (alloc_out_end, out_end))
            self.free_blocks.append(free2)
        
        # Bloco 3: Diagonal inferior direita (apenas se ambos sobrarem)
        if alloc_in_end < in_end and alloc_out_end < out_end:
            free3 = ((alloc_in_end, in_end), (alloc_out_end, out_end))
            self.free_blocks.append(free3)
        
        return True

    def get_layer(self, name: str):
        """Retorna os pesos e bias para uma camada alocada"""
        if name not in self.allocations:
            raise ValueError(f"AlocaÃ§Ã£o {name} nÃ£o encontrada")
        
        (in_start, in_end), (out_start, out_end) = self.allocations[name]
        
        weight = self.total_weight[out_start:out_end, in_start:in_end]
        bias = self.total_bias[out_start:out_end]
        
        return weight, bias

    def forward(self, x, allocation_name: str):
        """Executa a operaÃ§Ã£o linear usando os parÃ¢metros alocados"""
        weight, bias = self.get_layer(allocation_name)
        return F.linear(x, weight, bias)

    def print_memory_map(self):
        """FunÃ§Ã£o auxiliar para visualizar o mapa de memÃ³ria"""
        print("\nğŸ“Š Mapa de MemÃ³ria do Neuratron:")
        print(f"Peso total: {self.total_weight.shape}")
        print(f"AlocaÃ§Ãµes: {len(self.allocations)}")
        
        for name, ((in_s, in_e), (out_s, out_e)) in self.allocations.items():
            print(f"  {name}: entrada [{in_s}:{in_e}], saÃ­da [{out_s}:{out_e}]")
        
        print(f"Blocos livres: {len(self.free_blocks)}")
        for i, ((in_s, in_e), (out_s, out_e)) in enumerate(self.free_blocks):
            print(f"  Livre {i}: entrada [{in_s}:{in_e}], saÃ­da [{out_s}:{out_e}]")
